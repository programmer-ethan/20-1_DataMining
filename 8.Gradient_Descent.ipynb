{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scratch.linear_algebra import distance, subtract, scalar_multiply\n",
    "from functools import reduce\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v):\n",
    "    \"\"\"computes the sum of squared elements in v\"\"\"\n",
    "    return sum(v_i ** 2 for v_i in v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_quotient(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_estimated_derivative():\n",
    "    def square(x):\n",
    "        return x * x\n",
    "    def derivative(x):\n",
    "        return 2 * x\n",
    "    derivative_estimate = lambda x: difference_quotient(square, x, h=0.00001)\n",
    "# plot to show they're basically the same\n",
    "    import matplotlib.pyplot as plt\n",
    "    x = range(-10,10)\n",
    "    plt.plot(x, list(map(derivative, x)), 'rx', label='Actual') # red x\n",
    "    plt.plot(x, list(map(derivative_estimate, x)), 'b+', label='Estimate') # blue +\n",
    "    plt.legend(loc=9)\n",
    "    plt.title('Actual vs Estimate')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plot_estimated_derivative()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When f is a function of many variables, it has multiple partial derivatives, each indicating how f changes\n",
    "when we make small changes in just one of the input variables.\n",
    "We calculate its ith partial derivative by treating it as a function of just its ith variable, holding the other\n",
    "variables fixed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f, v, i, h):\n",
    " # add h to just the i-th element of v\n",
    "    w = [v_j + (h if j == i else 0)\n",
    "    for j, v_j in enumerate(v)]\n",
    "    return (f(w) - f(v)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i, _ in enumerate(v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.00001000001393, 2.00001000001393, 2.00001000001393]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_gradient(sum_of_squares, [1.,1.,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def estimate_gradient_np(f, v, h=0.00001):\n",
    "    return (np.apply_along_axis(f, 1, v + h * np.eye(v.shape[0])) - f(v)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.00001, 2.00001, 2.00001])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_gradient_np(lambda v: np.sum(v * v), np.array([1.,1.,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum_of_squares function is smallest when its input v is a vector of zeros.\n",
    "We want to verify the fact using gradient descent.\n",
    "(Gradient descent) Let’s use gradients to find the minimum among all three-dimensional vectors. We’ll\n",
    "just pick a random starting point and then take tiny steps in the opposite direction of the gradient until we\n",
    "reach a point where the gradient is very small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(v, direction, step_size):\n",
    "    \"\"\"move step_size in the direction from v\"\"\"\n",
    "    return [v_i + step_size * direction_i\n",
    "    for v_i, direction_i in zip(v, direction)]\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_np(v, direction, step_size):\n",
    "    \"\"\"move step_size in the direction from v\"\"\"\n",
    "    return v + step_size * direction\n",
    "def sum_of_squares_gradient_np(v):\n",
    "    return 2 * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using the gradient\n",
      "minimum v [-4.816976178530657e-07, 4.335278560677595e-06, -2.4084880892653316e-06]\n",
      "minimum value 2.4827487669849083e-11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"using the gradient\")\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "tolerance = 0.0000001\n",
    "while True:\n",
    " #print(v, sum_of_squares(v))\n",
    "    gradient = sum_of_squares_gradient(v) # compute the gradient at v\n",
    "    next_v = step(v, gradient, -0.01) # take a negative gradient step\n",
    "    if distance(next_v, v) < tolerance: # stop if we're converging\n",
    "        break\n",
    "    v = next_v # continue if we're not\n",
    "print(\"minimum v\", v)\n",
    "print(\"minimum value\", sum_of_squares(v))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the Right Step Size (or Learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the rationale for moving against the gradient is clear, how far to move is not. Indeed, choosing the\n",
    "right step size is more of an art than a science. Popular options include:\n",
    "Using a fixed step size\n",
    "Gradually shrinking the step size over time\n",
    "At each step, choosing the step size that minimizes the value of the objective function\n",
    "The last sounds optimal but is, in practice, a costly computation. We can approximate it by trying a variety of\n",
    "step sizes and choosing the one that results in the smallest value of the objective function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def sum_of_squares_gradient_np(v):\n",
    "    return 2 * v\n",
    "def gradient_descent(gradient_f, init_x, lr=0.01, step_num=10000, tolerance=0.0000001):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "    for i in range(step_num):\n",
    "        x_history.append(x.copy())\n",
    "        x_prev = x.copy()\n",
    "        x -= lr * gradient_f(x)\n",
    "        if np.linalg.norm(x - x_prev) < tolerance: # stop if we're converging\n",
    "            break\n",
    "    return x, x_history#!! indenting 잘못해서!!!\n",
    "    \n",
    "init_x = np.array([-1.0, 1.0])\n",
    "\n",
    "lr = 0.1# try with 10, 1.1, 1, 0.1, 0.01\n",
    "step_num = 100 \n",
    "x, x_history = gradient_descent(sum_of_squares_gradient_np, init_x, lr=lr, step_num=step_num)\n",
    "plt.plot( [-5, 5], [0,0], '--b')\n",
    "plt.plot( [0,0], [-5, 5], '--b')\n",
    "x_history = np.array(x_history)\n",
    "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe(f):\n",
    "    \"\"\"define a new function that wraps f and return it\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf') # this means \"infinity\" in Python\n",
    "    return safe_f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the general case, we have some target_fn that we want to minimize, and we also have its gradient_fn . For\n",
    "example, the target_fn could represent the errors in a model as a function of its parameters, and we\n",
    "might want to find the parameters that make the errors as small as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    \"\"\"use gradient descent to find theta that minimizes target function\"\"\"\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    theta = theta_0 # set theta to initial value\n",
    "    target_fn = safe(target_fn) # safe version of target_fn\n",
    "    value = target_fn(theta) # value we're minimizing\n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                    for step_size in step_sizes]\n",
    "        # choose the one that minimizes the error function\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "        # stop if we're \"converging\"\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example : Minimizing sum_of_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0006805647338418772,\n",
       " 0.0013611294676837543,\n",
       " 0.00027222589353675085,\n",
       " 0.0003402823669209386]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize_batch(sum_of_squares, sum_of_squares_gradient, [10,20,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0006805647338418772,\n",
       " 0.0013611294676837543,\n",
       " 0.00027222589353675085,\n",
       " 0.0003402823669209386,\n",
       " 0.0,\n",
       " 6.805647338418771e-05]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize_batch(sum_of_squares, sum_of_squares_gradient, [10,20,4,5,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example : Centering a certain point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0016059738814325, 2.000015426605225]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myf(v):\n",
    "    return (v[0]-3)**2 + (v[1]-2)**2\n",
    "def myf_gradient(v):\n",
    "    return [2.0*v[0]-6, 2.0*v[1]-4]\n",
    "minimize_batch(myf, myf_gradient, [5000.,50.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6.999843894783611, 69.99843894783609, 6.999843894783611, 3.9999107970192056]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "def f1(x, c):\n",
    "    x = np.array(x)\n",
    "    c = np.array(c)\n",
    "    return np.sum((x - c)**2)\n",
    "def f1_gradient(x, c):\n",
    "    x = np.array(x)\n",
    "    c = np.array(c)\n",
    "    return 2*x - 2*c\n",
    "def numerical_gradient(v, f, h=0.00001):\n",
    "    return (f(v) - np.apply_along_axis(f, 1, v - h * np.eye(len(v)))) / h\n",
    "c = np.array([7,70,7,4])\n",
    "f = partial(f1, c=c)\n",
    "#gradient_f = partial(numerical_gradient, f=f)\n",
    "gradient_f = partial(f1_gradient, c=c)\n",
    "minimize_batch(f, gradient_f, [0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negate(f):\n",
    "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "def negate_all(f):\n",
    "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                            negate_all(gradient_fn),\n",
    "                            theta_0,\n",
    "                            tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0023954427570109793]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "def normal_pdf(npx, mu, sigma):\n",
    "    x = npx[0]\n",
    "    return ((1/(np.sqrt(2*np.pi)*sigma)*np.exp(-(x-mu)**2/(2*sigma**2))))\n",
    "def numerical_gradient(v, f, h=0.00001):\n",
    "    return (np.apply_along_axis(f, 1, v + h * np.eye(len(v))) - f(v)) / h\n",
    "f = partial(normal_pdf, mu=0, sigma=1)\n",
    "gradient_f = partial(numerical_gradient, f=f)\n",
    "init_x = np.array([-4.])\n",
    "maximize_batch(f, gradient_f, init_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# minimize / maximize stochastic\n",
    "#\n",
    "def in_random_order(data):\n",
    "    \"\"\"generator that returns the elements of data in random order\"\"\"\n",
    "    indexes = [i for i, _ in enumerate(data)] # create a list of indexes\n",
    "    random.shuffle(indexes) # shuffle them\n",
    "    for i in indexes: # return the data in that order\n",
    "        yield data[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding SGD Code\n",
    "\n",
    "x is a training data set\n",
    "y is a label (or class) set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    data = list(zip(x, y))\n",
    "    theta = theta_0 # initial guess\n",
    "    alpha = alpha_0 # initial step size\n",
    "    min_theta, min_value = None, float(\"inf\") # the minimum so far\n",
    "    iterations_with_no_improvement = 0\n",
    "# if we ever go 100 iterations with no improvement, stop\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum( target_fn(x_i, y_i, theta) for x_i, y_i in data )\n",
    "        if value < min_value:\n",
    "        # if we've found a new minimum, remember it\n",
    "        # and go back to the original step size\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "# otherwise we're not improving, so try shrinking the step size\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "# and take a gradient step for each of the data points\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "    return min_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    return minimize_stochastic(negate(target_fn),\n",
    " negate_all(gradient_fn),\n",
    "x, y, theta_0, alpha_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
